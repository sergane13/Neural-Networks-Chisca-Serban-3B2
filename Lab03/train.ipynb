{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from torchvision.datasets import MNIST\n",
    "# def download_mnist(is_train: bool):\n",
    "#     dataset = MNIST(root='./data',\n",
    "#                     transform=lambda x: np.array(x).flatten(),\n",
    "#                     download=True,\n",
    "#                     train=is_train)\n",
    "#     mnist_data = []\n",
    "#     mnist_labels = []\n",
    "#     for image, label in dataset:\n",
    "#         mnist_data.append(image)\n",
    "#         mnist_labels.append(label)\n",
    "#     return mnist_data, mnist_labels\n",
    "# train_X, train_Y = download_mnist(True)\n",
    "# test_X, test_Y = download_mnist(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have numbers and we have them labeled\n",
    "# We normalize the data by applying a Min-Max Normalization\n",
    "\n",
    "normalized_data_train = (train_X - np.min(train_X)) / (np.max(train_X) - np.min(train_X))\n",
    "normalized_data_test = (test_X - np.min(test_X)) / (np.max(test_X) - np.min(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding\n",
    "# there are 10 labels \"0 -> 9\"\n",
    "\n",
    "numberOfClasses = 10\n",
    "\n",
    "one_hot_encoded_train = np.zeros((len(train_Y), numberOfClasses))\n",
    "one_hot_encoded_train[np.arange(len(train_Y)), train_Y] = 1\n",
    "\n",
    "one_hot_encoded_test = np.zeros((len(test_Y), numberOfClasses))\n",
    "one_hot_encoded_test[np.arange(len(test_Y)), test_Y] = 1\n",
    "\n",
    "# 5\n",
    "print(one_hot_encoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "(10,)\n",
      "(100, 784)\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "# Split the data for each epochs in batches of 100 elements\n",
    "\n",
    "def split_into_batches(data, batch_size):\n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    batches = np.array_split(data, num_batches)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "batchSize = 100\n",
    "\n",
    "print(normalized_data_train[0].shape)\n",
    "print(one_hot_encoded_train[0].shape)\n",
    "\n",
    "normalized_data_train = split_into_batches(normalized_data_train, batchSize)\n",
    "one_hot_encoded_train = split_into_batches(one_hot_encoded_train, batchSize)\n",
    "\n",
    "print(normalized_data_train[0].shape)\n",
    "print(one_hot_encoded_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the weights matrix with random alocation between 0 and 1\n",
    "\n",
    "weightsMatrix = np.random.rand(784, 10)\n",
    "biasMatrix = np.random.rand(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "# Forward propagation and softmax function\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def forwardPropagation(data, weights, bias):\n",
    "    z = np.dot(data, weights) + bias\n",
    "    return softmax(z)\n",
    "\n",
    "# def cross_entropy_loss(y_pred, y_true):\n",
    "#     y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "#     loss = -np.sum(y_true * np.log(y_pred), axis=1)\n",
    "#     return np.mean(loss)\n",
    "\n",
    "def gradient_descent_update(W, b, x, target, y, learning_rate):\n",
    "    error = target - y \n",
    "    W += learning_rate * np.dot(x.T, error)\n",
    "    b += learning_rate * np.sum(error, axis=0)\n",
    "\n",
    "    return W, b\n",
    "\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Number of epochs\n",
    "print(len(normalized_data_train))  # the split generated 600 batches\n",
    "\n",
    "max_iterations = 450\n",
    "iteration_count = 0\n",
    "\n",
    "for normalized_data_batch, one_hot_encoded_batch in zip(normalized_data_train, one_hot_encoded_train):\n",
    "    predictions = forwardPropagation(normalized_data_batch, weightsMatrix, biasMatrix)\n",
    "    # loss = cross_entropy_loss(predictions, one_hot_encoded_batch)\n",
    "    \n",
    "    weightsMatrix, biasMatrix = gradient_descent_update(weightsMatrix, biasMatrix, normalized_data_batch, one_hot_encoded_batch, predictions, learning_rate)\n",
    "    \n",
    "    iteration_count += 1\n",
    "    if iteration_count >= max_iterations:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "(10000, 10)\n",
      "0.8973\n"
     ]
    }
   ],
   "source": [
    "# Testing the NN\n",
    "\n",
    "print(normalized_data_test.shape)\n",
    "print(one_hot_encoded_test.shape)\n",
    "\n",
    "def calculate_accuracy(predictions, one_hot_labels):\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    actual_classes = np.argmax(one_hot_labels, axis=1) \n",
    "\n",
    "    return np.mean(predicted_classes == actual_classes)\n",
    "\n",
    "test_predictions = forwardPropagation(normalized_data_test, weightsMatrix, biasMatrix)\n",
    "accuracy = calculate_accuracy(test_predictions, one_hot_encoded_test)\n",
    "\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
