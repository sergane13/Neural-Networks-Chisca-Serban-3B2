{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def download_mnist(is_train: bool):\n",
    "    dataset = MNIST(root='./data',\n",
    "                    transform=lambda x: np.array(x).flatten(),\n",
    "                    download=True,\n",
    "                    train=is_train)\n",
    "    mnist_data = []\n",
    "    mnist_labels = []\n",
    "    for image, label in dataset:\n",
    "        mnist_data.append(image)\n",
    "        mnist_labels.append(label)\n",
    "    return mnist_data, mnist_labels\n",
    "train_X, train_Y = download_mnist(True)\n",
    "test_X, test_Y = download_mnist(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have numbers and we have them labeled\n",
    "# We normalize the data by applying a Min-Max Normalization\n",
    "\n",
    "normalized_data_train = (train_X - np.mean(train_X)) / np.std(train_X)\n",
    "normalized_data_test = (test_X - np.mean(test_X)) / np.std(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# One-Hot Encoding\n",
    "# there are 10 labels \"0 -> 9\"\n",
    "\n",
    "numberOfClasses = 10\n",
    "\n",
    "one_hot_encoded_train = np.zeros((len(train_Y), numberOfClasses))\n",
    "one_hot_encoded_train[np.arange(len(train_Y)), train_Y] = 1\n",
    "\n",
    "one_hot_encoded_test = np.zeros((len(test_Y), numberOfClasses))\n",
    "one_hot_encoded_test[np.arange(len(test_Y)), test_Y] = 1\n",
    "\n",
    "# 5\n",
    "print(one_hot_encoded_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "(10,)\n",
      "(100, 784)\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "def split_into_batches(data, batch_size):\n",
    "    num_batches = data.shape[0] // batch_size \n",
    "    batches = np.array_split(data, num_batches)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "batchSize = 100\n",
    "\n",
    "print(normalized_data_train[0].shape)\n",
    "print(one_hot_encoded_train[0].shape)\n",
    "\n",
    "normalized_data_train = split_into_batches(normalized_data_train, batchSize)\n",
    "one_hot_encoded_train = split_into_batches(one_hot_encoded_train, batchSize)\n",
    "\n",
    "print(normalized_data_train[0].shape)\n",
    "print(one_hot_encoded_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights with He initialization\n",
    "W1 = np.random.randn(784, 100) * np.sqrt(2 / 784)\n",
    "W2 = np.random.randn(100, 10) * np.sqrt(2 / 100)\n",
    "\n",
    "# Initialize biases\n",
    "b1 = np.zeros((100,))\n",
    "b2 = np.zeros((10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hidden layer\n",
    "def relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def relu_derivative(z, alpha=0.01):\n",
    "    return np.where(z > 0, 1, alpha)\n",
    "\n",
    "# for output layer\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forwardPropagation(data, weightsL1, biasL1, weightsL2, biasL2):\n",
    "    hidden = np.dot(data, weightsL1) + biasL1\n",
    "    reluz_hidden = relu(hidden)\n",
    "    \n",
    "    output = np.dot(reluz_hidden, weightsL2) + biasL2\n",
    "    return softmax(output), reluz_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(one_hot_labels, predictions):\n",
    "    predictions = np.clip(predictions, 1e-10, 1.0)\n",
    "    return -np.mean(np.sum(one_hot_labels * np.log(predictions), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularization(weights, lambda_val):\n",
    "    return lambda_val * np.sum(weights ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 0.414423152412823, Test Accuracy: 0.28%\n",
      "Epoch 2/200, Loss: 0.3419255565631778, Test Accuracy: 0.20%\n",
      "Epoch 3/200, Loss: 0.3066146235111716, Test Accuracy: 0.16%\n",
      "Epoch 4/200, Loss: 0.28532693438050644, Test Accuracy: 0.16%\n",
      "Epoch 5/200, Loss: 0.2702665585076447, Test Accuracy: 0.16%\n",
      "Epoch 6/200, Loss: 0.2581731137958874, Test Accuracy: 0.14%\n",
      "Epoch 7/200, Loss: 0.24842842246209831, Test Accuracy: 0.20%\n",
      "Epoch 8/200, Loss: 0.2411512370585035, Test Accuracy: 0.21%\n",
      "Epoch 9/200, Loss: 0.23502715467180263, Test Accuracy: 0.25%\n",
      "Epoch 10/200, Loss: 0.22951616154441393, Test Accuracy: 0.24%\n",
      "Epoch 11/200, Loss: 0.22455402923974124, Test Accuracy: 0.23%\n",
      "Epoch 12/200, Loss: 0.2198177457069336, Test Accuracy: 0.23%\n",
      "Epoch 13/200, Loss: 0.2162459123110598, Test Accuracy: 0.23%\n",
      "Epoch 14/200, Loss: 0.21297554922662582, Test Accuracy: 0.24%\n",
      "Epoch 15/200, Loss: 0.2102353163643673, Test Accuracy: 0.24%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[743], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m normalized_data_batch, one_hot_encoded_batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(normalized_data_train, one_hot_encoded_train):\n\u001b[0;32m----> 7\u001b[0m         activation_L2, activation_L1 \u001b[38;5;241m=\u001b[39m \u001b[43mforwardPropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_data_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m         loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(one_hot_encoded_batch, activation_L2)\n\u001b[1;32m     11\u001b[0m         m \u001b[38;5;241m=\u001b[39m activation_L2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[740], line 3\u001b[0m, in \u001b[0;36mforwardPropagation\u001b[0;34m(data, weightsL1, biasL1, weightsL2, biasL2)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforwardPropagation\u001b[39m(data, weightsL1, biasL1, weightsL2, biasL2):\n\u001b[0;32m----> 3\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweightsL1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m biasL1\n\u001b[1;32m      4\u001b[0m     reluz_hidden \u001b[38;5;241m=\u001b[39m relu(hidden)\n\u001b[1;32m      6\u001b[0m     output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(reluz_hidden, weightsL2) \u001b[38;5;241m+\u001b[39m biasL2\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "learning_rate = 0.01\n",
    "lambda_val = 0.0001 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for normalized_data_batch, one_hot_encoded_batch in zip(normalized_data_train, one_hot_encoded_train):\n",
    "        activation_L2, activation_L1 = forwardPropagation(normalized_data_batch, W1, b1, W2, b2)\n",
    "        \n",
    "        loss = cross_entropy_loss(one_hot_encoded_batch, activation_L2)\n",
    "\n",
    "        m = activation_L2.shape[0]\n",
    "        \n",
    "        # Output layer\n",
    "        error_output_layer = one_hot_encoded_batch - activation_L2\n",
    "        d_W2 = np.dot(activation_L1.T, error_output_layer) / m\n",
    "        d_b2 = np.sum(error_output_layer, axis=0) / m\n",
    "        \n",
    "        # Hiddem layer\n",
    "        error_hidden_layer = np.dot(error_output_layer, W2.T) * relu_derivative(activation_L1)\n",
    "        d_W1 = np.dot(normalized_data_batch.T, error_hidden_layer) / m\n",
    "        d_b1 = np.sum(error_hidden_layer, axis=0) / m\n",
    "        \n",
    "        # L2 regularization\n",
    "        d_W1 += lambda_val * W1 / m\n",
    "        d_W2 += lambda_val * W2 / m\n",
    "        \n",
    "        W1 += learning_rate * d_W1\n",
    "        b1 += learning_rate * d_b1\n",
    "        W2 += learning_rate * d_W2\n",
    "        b2 += learning_rate * d_b2\n",
    "        \n",
    "    _, test_predictions = forwardPropagation(normalized_data_test, W1, b1, W2, b2)\n",
    "    test_accuracy = calculate_accuracy(test_predictions, one_hot_encoded_test)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "    \n",
    "print(np.isnan(W1).any(), np.isnan(W2).any())\n",
    "print(np.isnan(activation_L1).any(), np.isnan(activation_L2).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.01%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(predictions, one_hot_labels):\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    actual_classes = np.argmax(one_hot_labels, axis=1) \n",
    "\n",
    "    return np.mean(predicted_classes == actual_classes) * 100\n",
    "\n",
    "_, test_predictions  = forwardPropagation(normalized_data_test, W1, b1, W2, b2)\n",
    "accuracy = calculate_accuracy(test_predictions, one_hot_encoded_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
